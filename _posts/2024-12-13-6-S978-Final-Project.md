---
layout: post
title: Evaluating Heavy-Tailed Diffusion Models on Naturally Heavy-Tailed Datasets with Rare Event Statistics
subtitle : 6.S978 Final Project
tags: [Course Project]
author: Kai Chang, David Jin
comments : True
---

## Introduction
In its broadest sense, generative modeling addresses the statistical challenge of estimating and sampling from an unknown probability distribution given only a finite set of samples. When the target distribution is high-dimensional, this task becomes daunting, and traditional approaches such as Gaussian Mixture Models quickly become computationally infeasible [1]. Modern generative modeling techniques have largely circumvented these limitations by focusing on learning a mapping from a simple base distribution, which is easy to sample from, to the target distribution, approximating this mapping using the available data. The specifics of how this mapping is defined and constructed vary significantly across methods. Over the past decade, several approaches leveraging deep neural networks have emerged, including Variational Autoencoders (VAEs) [2], Generative Adversarial Networks (GANs) [3], Normalizing Flows [4], Denoising Diffusion Probabilistic Models (DDPMs) [5], and Score-Based Diffusion Models (SBMs) [6].

Among these frameworks, SBMs and DDPMs have achieved remarkable success across a wide range of applications, from image generation and protein design to weather forecasting, robotic manipulation, and climate emulation. SBMs model the mapping from the base to the target distribution as the reverse stochastic differential equation (SDE) of a pre-defined forward Ornstein-Uhlenbeck (OU) process, while DDPMs can be seen as a discrete counterpart of SBMs. In these models, the forward process gradually transforms samples from the target distribution into the base distribution by adding Gaussian noise with specified covariance structures. The reverse process similarly involves adding Gaussian noise to samples from the base distribution, effectively producing samples that mimic the target distribution. However, due to the nature of the OU process, the added noise is inherently Gaussian.

While this Gaussian constraint works well in many scenarios, it can be limiting when the target distribution exhibits heavy-tailed (rare and extreme events) or long-tailed (imbalanced datasets) characteristics. Recent works argue that Gaussian noise, with its square-exponentially decaying tails, inherently biases the generation process toward frequent, central samples in the training data, often underrepresenting rare or extreme samples that are at least equally critical. This limitation in diversity poses challenges in domains where extreme events or class imbalances are naturally present, as observed in various scientific and engineering contexts (e.g., [8]).

To address this limitation, recent studies have explored replacing Gaussian noise and Brownian motions with heavy-tailed noise or stochastic processes [7, 12, 13, 14]. While these works report improvements using metrics such as the Fréchet Inception Distance (FID) and Inception Score, these metrics, widely used in image generation tasks, are not particularly suited to assessing whether generative models effectively capture the tails of a distribution. In our view, this represents a significant gap in the current literature.

Motivated by this gap, we propose an alternative evaluation framework grounded in mathematical rigor and physical intuition, aimed at better assessing the impact of heavy-tailed noise on capturing rare and extreme events. Our approach consists of four steps: (1) selecting a dataset that naturally exhibits heavy tails, such as ERA5 precipitation data, (2) identifying a physical aggregate quantity that characterizes the heavy-tail phenomenon, such as order statistics, (3) comparing the probability density of this aggregate quantity for both the original dataset and the samples generated by the model, and (4) computing tail-emphasizing statistics, such as those developed in [9, 10], to quantify differences. In particular, we focus on a representative type of heavy-tailed diffusion models (HTDM), the Lévy-Itō Models, and compare its ability to reproduce the tail events with SBMs when they are both trained on precipitation data. 

Since this blog prioritizes accessibility over formalism, we focus on intuition and provide sufficient mathematical background for clarity. Rather than claiming that our approach is state-of-the-art, we aim to demonstrate the value of designing evaluation strategies from first principles. We hope this exposition inspires readers to reconsider the ways in which generative models are evaluated, particularly in contexts involving rare and extreme events.

## Problem Statement
Let $p_0$ denote the true data distribution from which we have access to independent and identically distributed (IID) samples $\\{x_i\\}_{i=1}^n$, where $x(0)$ represents the corresponding random variable. Similarly, let $p_T$ be a known and tractable distribution from which samples can be easily generated, and let $x(T)$ denote its corresponding random variable. 

The goal of generative modeling is to learn a mapping $\tau$ such that the push-forward distribution of $p_T$ under $\tau$ matches $p_0$, expressed mathematically as:
\\[\tau_{\\#} p_T = p_0\\]
 where $\tau_{\\#} p_T$ represents the push-forward of $p_T$ by $\tau$. Intuitively, this means that for a sample $\xi \sim p_T$, applying the map $\tau$ transforms it into a sample $\tau(\xi)$ that follows $p_0$.

## Score-Based Diffusion Models

Score-based diffusion models (SBMs) define the mapping $\tau$ using an Ornstein-Uhlenbeck (OU) process, as illustrated in Figure 1.

|![sample image]({{ site.baseurl }}/assets/img/2024-12-13-6-S978-Final-Project/figure1.jpg)|
|:--:| 
| *Figure 1* |

 SBMs consist of two key processes. In the forward process, each $x_i \sim p_0$ is progressively transformed into $\xi_i \sim p_T$ by adding Gaussian noise at each step. In the backward process, a sample $\xi \sim p_T$ is iteratively refined through a reverse process to produce $x \sim p_0$, effectively reconstructing the original data distribution.

It is important to note that Gaussian noise is applied during both the forward and backward processes. Figure 2 [homework] provides a visualization of the OU diffusion process, demonstrating the transformation of a bimodal Gaussian distribution into a unimodal Gaussian distribution in 1-D.

|![sample image]({{ site.baseurl }}/assets/img/2024-12-13-6-S978-Final-Project/figure2.png)|
|:--:| 
| *Figure 2* |

## The Lévy-Itō Model

While SBMs have demonstrated outstanding performance across many applications, recent studies have highlighted their limitations in effectively covering the true distribution when the training dataset is imbalanced—for example, if 90% of the images are of dogs and only 10% are of cats. This limitation arises because Gaussian noise has light tails, causing sample trajectories to concentrate around frequent samples in the training data while neglecting under-represented samples. This imbalance often results in poor coverage of minority classes. Motivated by this drawback, recent works have proposed the use of heavy-tailed noise in the measure-transportation process.

To intuitively understand the advantage of heavy-tailed noise over Gaussian noise, consider a comparison between a standard Gaussian distribution and a standard Cauchy distribution. Figure 3 illustrates the 90% centered probability coverage for each distribution. The range of values covered by the Cauchy distribution is more than twice that of the Gaussian, reflecting the heavier tail of the Cauchy distribution. This wider range implies that samples from the Cauchy distribution exhibit greater diversity compared to those from the Gaussian distribution. Incorporating such heavy-tailed noise into the diffusion process is the fundamental idea behind heavy-tailed diffusion models.

|![sample image]({{ site.baseurl }}/assets/img/2024-12-13-6-S978-Final-Project/figure3.jpg)|
|:--:| 
| *Figure 3* |  

To explore this concept, we examine a representative model in this category: the Lévy-Itō Model (LIM). LIM replaces Gaussian noise with $\alpha$-stable noise, which has heavier tails. In the continuous case, the sample-transporting trajectories correspond to a Lévy process (when $\alpha < 2$) rather than a Brownian motion. Figure 4 provides an illustration of the LIM framework, and Figure 5 visualizes sample trajectories in a 1D-to-1D toy example.

|![sample image]({{ site.baseurl }}/assets/img/2024-12-13-6-S978-Final-Project/figure4.jpg)|
|:--:| 
| *Figure 4* | 
|![sample image]({{ site.baseurl }}/assets/img/2024-12-13-6-S978-Final-Project/figure5.jpg)|
|:--:| 
| *Figure 5* | 

## Existing Evaluations

Many claims about the effectiveness of heavy-tailed diffusion models center on their purported ability to improve the coverage of the data distribution. However, to the best of our knowledge, these claims are predominantly supported by reductions in numerical scores from metrics such as the Fréchet Inception Distance (FID). Figure 6 provides an illustration of how FID is computed and used.

|![sample image]({{ site.baseurl }}/assets/img/2024-12-13-6-S978-Final-Project/figure6.jpg)|
|:--:| 
| *Figure 6* | 

At its core, FID quantifies the similarity between the real data distribution and the distribution of samples generated by a model. It achieves this as follows. Let $\\{x_i\\}^n_{i=1}$ represent the real data and $\\{y_i\\}^m_{i=1}$ the model-generated samples. Using a high-fidelity feature extractor $f$, such as the Inception v3 model pretrained on ImageNet [15, 16], the features of $x_i$ and $y_j$ are obtained as:
\\[f_i = f(x_i) \quad \text{and} \quad g_j = f(y_j), \quad \forall \, i, \, j.\\]
The empirical mean and covariance of the features for the real data are computed as:
\\[\mu_r = \frac{1}{n} \sum_{i=1}^n f_i, \quad \Sigma_r = \frac{1}{n} \sum_{i=1}^n (f_i - \mu_r)(f_i - \mu_r)^T.\\]
Similarly, the empirical mean and covariance of the features for the generated samples are:
\\[\mu_g = \frac{1}{m} \sum_{j=1}^{m} g_j, \quad \Sigma_g = \frac{1}{m} \sum_{j=1}^{m} (g_j - \mu_g)(g_j - \mu_g)^T.\\]
The FID is then defined as the 2-Wasserstein distance between the two Gaussian distributions $\mathcal{N}(\mu_r, \Sigma_r)$ and $\mathcal{N}(\mu_g, \Sigma_g)$, given by:
\\[\text{FID} = \|\mu_r - \mu_g\|_2^2 + \text{Tr}(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2}),\\]
where $\|\cdot\|_2$ denotes the Euclidean norm, $\text{Tr}$ is the trace operator, and $(\Sigma_r \Sigma_g)^{1/2}$ represents the matrix square root of the product $\Sigma_r \Sigma_g$.

While FID has become the standard evaluation metric for image generation tasks, we argue that it falls short in assessing whether heavy-tailed diffusion models truly extend their coverage into the tail regions of the data distribution, due to the following limitations.

1. **Focus on Central Tendency**: FID primarily measures the alignment of the central moments (mean and covariance) of the distributions. It is not sensitive to rare samples or the behavior of the tails of the distributions.

2. **Gaussian Assumption**: By fitting Gaussians to the features, FID inherently disregards any non-Gaussian characteristics, which are critical when evaluating models designed to capture heavy tails.

3. **Feature Extractor Bias**: The choice of the feature extractor $f$ can heavily influence FID scores. If $f$ is not sensitive to tail samples, FID cannot adequately measure coverage in the tails.

4. **Aggregate Nature**: FID provides a single numerical score that does not distinguish between improvements in common regions of the distribution and improvements in rare or extreme regions.

Given these limitations, we believe that more targeted evaluation strategies are necessary to assess the ability of heavy-tailed diffusion models to capture rare and extreme events in the data distribution. In the next section, we propose a new evaluation framework designed to address these shortcomings.

## A Physically Interpretable Evaluation Strategy

## Results

## Discussion and Conclusion

## References

[1] Bishop, Christopher M., and Nasser M. Nasrabadi. Pattern recognition and machine learning. Vol. 4. No. 4. New York: springer, 2006.

[2] Kingma, Diederik P., and Max Welling. "Auto-Encoding Variational Bayes." 2nd International Conference on Learning Representations (ICLR), 2014.

[3] Goodfellow, Ian, et al. "Generative adversarial nets." Advances in neural information processing systems 27 (2014).

[4] Rezende, Danilo, and Shakir Mohamed. "Variational inference with normalizing flows." International conference on machine learning. PMLR, 2015.

[5] Ho, Jonathan, Ajay Jain, and Pieter Abbeel. "Denoising diffusion probabilistic models." Advances in neural information processing systems 33 (2020): 6840-6851.

[6] Song, Yang, et al. "Score-based generative modeling through stochastic differential equations." arXiv preprint arXiv:2011.13456 (2020).

[7] Yoon, Eun Bi, et al. "Score-based generative models with Lévy processes." Advances in Neural Information Processing Systems 36 (2023): 40694-40707.

[8] Coles, Stuart, et al. An introduction to statistical modeling of extreme values. Vol. 208. London: Springer, 2001.

[9] Mohamad, Mustafa A., and Themistoklis P. Sapsis. "Sequential sampling strategy for extreme event statistics in nonlinear dynamical systems." Proceedings of the National Academy of Sciences 115.44 (2018): 11138-11143.

[10] Blanchard, Antoine, et al. "A multi-scale deep learning framework for projecting weather extremes." arXiv preprint arXiv:2210.12137 (2022).

[11] Heusel, Martin, et al. "Gans trained by a two time-scale update rule converge to a local nash equilibrium." Advances in neural information processing systems 30 (2017).

[12] Kushagra Pandey, Jaideep Pathak, Yilun Xu, Stephan Mandt, Michael Pritchard, Arash Vahdat, and Morteza Mardani. Heavy-Tailed Diffusion Models, October 2024. arXiv:2410.14171.

[13] Dario Shariatian, Umut Simsekli, and Alain Durmus. Denoising Lévy Probabilistic Models, July 2024. arXiv:2407.18609 [cs, stat].

[14] Eliya Nachmani, Robin San Roman, and Lior Wolf. Denoising Diffusion Gamma Models, October 2021. arXiv:2110.05948 [cs, eess].

[15] Szegedy, Christian, et al. "Rethinking the inception architecture for computer vision." Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.

[16] Deng, Jia, et al. "Imagenet: A large-scale hierarchical image database." 2009 IEEE conference on computer vision and pattern recognition. Ieee, 2009.
